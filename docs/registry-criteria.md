# Registry Criteria (Draft v0.1)
**Solana (Rust) Educator Profiles**

**Status:** Draft — In Development  
**Owner:** STAR Network  
**Scope:** Early criteria used to describe and evaluate educator profiles for potential registry inclusion

---

## 1. Purpose of This Document

This document outlines the **early registry criteria** STAR Network uses to describe and evaluate educator profiles in the context of **Solana (Rust) education**.

It exists to:
- Make verification logic transparent and interpretable  
- Establish consistent language for educator credibility  
- Support structured human review during early phases  
- Inform future Train-the-Trainer and benchmarking pathways  

This document is **not** a certification standard and does not establish enforceable requirements.

---

## 2. What “Registry Inclusion” Means (Early Phase)

Registry inclusion is intended to be a **neutral verification signal** indicating that an educator’s profile has been reviewed against documented criteria and reflects **observable teaching and technical capability** relevant to Solana (Rust) education.

Registry inclusion is not:
- a license  
- a guarantee of training outcomes  
- an endorsement of a single teaching style  
- an official Solana approval  
- a replacement for enterprise or institutional due diligence  

---

## 3. Principles of Evaluation

STAR Network applies the following principles when reviewing educator profiles:

### 3.1 Evidence Over Claims
Claims of expertise are interpreted through the lens of **observable and verifiable indicators**.

### 3.2 Teaching Capability Matters Separately
Technical skill and teaching skill are evaluated as distinct dimensions.

### 3.3 Context and Audience Fit
Criteria are interpreted relative to:
- learner level (beginner to advanced)
- training format (workshop, cohort-based, enterprise sessions)
- goals (onboarding, upskilling, specialized topics)

### 3.4 Neutrality and Transparency
The criteria are designed to avoid pay-to-play, popularity contests, or opaque ranking systems.

---

## 4. Core Registry Criteria (Exploratory)

The criteria below describe dimensions that STAR Network may consider during human review.  
They are **not** a checklist that guarantees inclusion.

---

### 4.1 Solana (Rust) Technical Relevance

The educator’s technical experience is evaluated for relevance to Solana (Rust) development.

Examples of signals that may support this dimension:
- demonstrated familiarity with Solana program development concepts  
- demonstrated Rust competence in a Solana context  
- ability to explain system-level constraints and trade-offs clearly  
- evidence of continued engagement with ecosystem changes and tools  

This dimension focuses on **relevance and clarity**, not prestige.

---

### 4.2 Teaching & Instructional Experience

The educator’s ability to teach is evaluated independently of technical experience.

Signals may include:
- history of delivering structured learning sessions  
- clarity and pacing in explanation  
- ability to answer questions and address misconceptions  
- ability to support learners through exercises and problem-solving  

The intent is to establish teaching capability as a first-class dimension of quality.

---

### 4.3 Pedagogical Quality (Method and Structure)

STAR Network considers whether teaching appears to be grounded in effective pedagogy.

Signals may include:
- use of progressive complexity and layered learning  
- emphasis on mental models before implementation details  
- structured exercises and “learning by doing”  
- ability to adapt explanations across learner levels  

This dimension aligns with the pedagogy principles documented in the standards framework.

---

### 4.4 Professional Practice & Reliability

Registry credibility also depends on professionalism and reliability.

Signals may include:
- clear communication and expectation-setting  
- preparedness and consistency in delivery  
- respect for learner context and constraints  
- transparency about what is known vs unknown  
- ethical boundaries and neutral conduct  

This dimension is essential for enterprise and institutional trust.

---

## 5. Supporting Information (Profile Completeness)

In early phases, STAR Network may also consider whether a profile contains sufficient clarity for interpretation.

A complete profile typically includes:
- educator focus areas within Solana (Rust)  
- teaching formats supported (workshops, cohorts, etc.)  
- prior teaching contexts (community, enterprise, institutional)  
- availability and engagement preferences  
- clear scope boundaries (what they teach, what they don’t)

This supports discovery and coordination without implying endorsement.

---

## 6. What STAR Network Avoids (By Design)

To preserve neutrality and long-term trust, STAR Network avoids:

- public star ratings or influencer-based ranking  
- automated scoring as a substitute for human review  
- “top trainer” leaderboards  
- unverifiable claims of mastery  
- treating marketing reach as a quality signal  

If reputation signals are introduced later, they must be structured and governed responsibly.

---

## 7. Relationship to Satisfaction Signals (Future-State)

STAR Network is designed to progress toward a registry model that can incorporate **satisfaction signals** and verified teaching history over time.

In early phases:
- satisfaction signals are not treated as a single objective truth  
- feedback interpretation requires context  
- signals must be collected and represented carefully  

Any formalized satisfaction methodology will be documented separately as governance matures.

---

## 8. Relationship to On-Chain Credentials (Future-State)

STAR Network is designed to progress toward a blockchain-backed registry where educators may hold:
- public profiles  
- verifiable teaching history  
- credential signals issued on Solana  

However:
- the registry does **not** begin as a fully on-chain system  
- blockchain components are introduced gradually  
- credential issuance requires mature governance and clear standards  

This document focuses on the foundational criteria that make future credentialing meaningful.

---

## 9. Status Labels (Informational)

During early phases, registry interpretation may use non-numeric labels such as:
- “In Review”
- “Verified (Early Phase)”
- “Inactive / Not Currently Reviewed”

These labels are informational and designed to improve transparency rather than create rankings.

---

## 10. Draft Status & Iteration Plan

This is **Draft v0.1**.

Expected evolution includes:
- refinement based on observed educator outcomes  
- alignment with Train-the-Trainer pilots  
- clearer separation between descriptive criteria and benchmarked evaluation  
- governance review before any certification pathway is introduced  

Updates will prioritize clarity and transparency over completeness.

---

## 11. Interpretation Notice

This document is a transparency artifact and is non-binding.

It should not be interpreted as:
- an enforceable standard  
- a certification framework  
- a guarantee of educator performance  
- an official ecosystem endorsement  

Its goal is to make educator credibility **understandable and reviewable** within a neutral registry model.

---

### End of Draft v0.1
